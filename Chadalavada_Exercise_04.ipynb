{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The fourth in-class-exercise (40 points in total, 03/28/2022)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\soumya\n",
      "[nltk_data]     nanditha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\soumya nanditha\\anaconda3\\lib\\site-packages\\gensim\\topic_coherence\\direct_confirmation_measure.py:202: RuntimeWarning: invalid value encountered in true_divide\n",
      "  numerator = (co_occur_count / num_docs) + EPSILON\n",
      "C:\\Users\\soumya nanditha\\anaconda3\\lib\\site-packages\\gensim\\topic_coherence\\direct_confirmation_measure.py:203: RuntimeWarning: invalid value encountered in true_divide\n",
      "  denominator = (w_prime_count / num_docs) * (w_star_count / num_docs)\n",
      "C:\\Users\\soumya nanditha\\anaconda3\\lib\\site-packages\\gensim\\topic_coherence\\direct_confirmation_measure.py:198: RuntimeWarning: invalid value encountered in true_divide\n",
      "  co_doc_prob = co_occur_count / num_docs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of topics (K): 4\n",
      "Topics:\n",
      "(0, '0.108*\"musk\" + 0.108*\"fund\" + 0.108*\"billionaire\" + 0.108*\"elon\" + 0.108*\"mars\"')\n",
      "(1, '0.109*\"government\" + 0.108*\"warns\" + 0.108*\"breaking:\" + 0.108*\"alien\" + 0.108*\"imminent,\"')\n",
      "(2, '0.108*\"disease\" + 0.108*\"effective\" + 0.108*\"vaccines\" + 0.108*\"study\" + 0.108*\"preventing\"')\n",
      "(3, '0.118*\"files\" + 0.118*\"leaked\" + 0.118*\"government\" + 0.118*\"top-secret\" + 0.118*\"exclusive:\"')\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "# importing the gensim module to use the LDA modelling technique\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "# importing the nlp tool kit and downloading the stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# input data to use\n",
    "news_input = [\"Breaking: Alien Invasion Imminent, Government Warns\",\n",
    "    \"Study Finds Vaccines Effective in Preventing Disease\",\n",
    "    \"Billionaire Elon Musk to Fund Mission to Mars\",\n",
    "    \"Exclusive: Top-Secret Government Files Leaked\",]\n",
    "\n",
    "# removing the stop words from input text and tokenizing the text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_text = [[word for word in doc.lower().split() if word not in stop_words] for doc in news_input]\n",
    "\n",
    "# Creating dictionary and corpus using the tokenized text\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_text]\n",
    "\n",
    "# Using coherence score parameter to decide number of topics\n",
    "coherence_scores = []\n",
    "for k in range(4, 16):  \n",
    "    lda_model = LdaModel(corpus=corpus, num_topics=k, id2word=dictionary)\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=tokenized_text, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    coherence_scores.append((k, coherence_score))\n",
    "\n",
    "# Find the K with the highest coherence score\n",
    "best_k, best_coherence = max(coherence_scores, key=lambda x: x[1])\n",
    "\n",
    "# Train the final LDA model with the best K\n",
    "final_lda_model = LdaModel(corpus=corpus, num_topics=best_k, id2word=dictionary)\n",
    "\n",
    "# Summarize the topics\n",
    "topics = final_lda_model.print_topics(num_words=5)  # You can change the number of words in topics\n",
    "\n",
    "# Print the best K and the topics\n",
    "print(\"Optimal number of topics (K):\", best_k)\n",
    "print(\"Topics:\")\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: government, warns, breaking, imminent, invasion\n",
      "Topic 2: to, in, preventing, finds, vaccines\n",
      "Topic 3: to, mission, mars, fund, elon\n",
      "Topic 4: leaked, top, exclusive, files, secret\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "news_input = [\"Breaking: Alien Invasion Imminent, Government Warns\",\n",
    "    \"Study Finds Vaccines Effective in Preventing Disease\",\n",
    "    \"Billionaire Elon Musk to Fund Mission to Mars\",\n",
    "    \"Exclusive: Top-Secret Government Files Leaked\",]\n",
    "\n",
    "# Step 1: Preprocess the text data and create a TF-IDF matrix\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(news_input)\n",
    "\n",
    "# Step 2: Determine the optimal number of topics (K) using LSA (you specify K)\n",
    "K = 6  \n",
    "lsa = TruncatedSVD(n_components=K)\n",
    "lsa_topic_matrix = lsa.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Step 3: Summarize the topics\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "topic_keywords = []\n",
    "for i, topic in enumerate(lsa.components_):\n",
    "    top_terms = [terms[idx] for idx in topic.argsort()[-5:][::-1]]\n",
    "    topic_keywords.append(top_terms)\n",
    "    print(f\"Topic {i + 1}: {', '.join(top_terms)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
    "\n",
    "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lda2vec\n",
      "  Downloading lda2vec-0.16.10.tar.gz (13 kB)\n",
      "Building wheels for collected packages: lda2vec\n",
      "  Building wheel for lda2vec (setup.py): started\n",
      "  Building wheel for lda2vec (setup.py): finished with status 'done'\n",
      "  Created wheel for lda2vec: filename=lda2vec-0.16.10-py3-none-any.whl size=14434 sha256=e2401badef4cd6e16f7a6e2e6c689a5e92956ce7c9e4083788fd5a854f8e1cf9\n",
      "  Stored in directory: c:\\users\\soumya nanditha\\appdata\\local\\pip\\cache\\wheels\\fa\\ad\\6c\\38aa944b34a94fd5d4f4d48e7432f94cd97f18d15779bdc9e5\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully built lda2vec\n",
      "Installing collected packages: lda2vec\n",
      "Successfully installed lda2vec-0.16.10\n"
     ]
    }
   ],
   "source": [
    "pip install lda2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lda2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Write your code here\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlda2vec\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Prepare and clean your text data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create word embeddings using Word2Vec or similar method\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize the LDA2Vec model\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lda2vec'"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "import lda2vec\n",
    "import numpy as np\n",
    "\n",
    "news_input = [\"Breaking: Alien Invasion Imminent, Government Warns\",\n",
    "    \"Study Finds Vaccines Effective in Preventing Disease\",\n",
    "    \"Billionaire Elon Musk to Fund Mission to Mars\",\n",
    "    \"Exclusive: Top-Secret Government Files Leaked\",]\n",
    "\n",
    "\n",
    "# Initialize the LDA2Vec model\n",
    "model = lda2vec.Lda2Vec(num_topics=K, num_words=len(vocabulary), min_df=5)\n",
    "\n",
    "\n",
    "top_n = 10\n",
    "topic_to_topwords = {}\n",
    "for j, topic_to_word in enumerate(dat['topic_term_dists']):\n",
    "    top = np.argsort(topic_to_word)[::-1][:top_n]\n",
    "    msg = 'Topic %i '  % j\n",
    "    top_words = [dat['vocab'][i].strip()[:35] for i in top]\n",
    "    msg += ' '.join(top_words)\n",
    "    print msg\n",
    "    topic_to_topwords[j] = top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
    "\n",
    "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Using cached bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
      "Collecting hdbscan>=0.8.29\n",
      "  Using cached hdbscan-0.8.33.tar.gz (5.2 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from bertopic) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from bertopic) (1.0.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from bertopic) (5.6.0)\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Using cached umap_learn-0.5.4-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from bertopic) (4.64.0)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from bertopic) (1.4.2)\n",
      "Collecting sentence-transformers>=0.4.1\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.7.3)\n",
      "Requirement already satisfied: cython<3,>=0.27 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (0.29.28)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: six in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (1.16.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.2.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (1.8.0)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "Requirement already satisfied: torchvision in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.5.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.3.15)\n",
      "Collecting tokenizers<0.15,>=0.14\n",
      "  Using cached tokenizers-0.14.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.4.0-cp39-none-win_amd64.whl (277 kB)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Collecting pynndescent>=0.5\n",
      "  Using cached pynndescent-0.5.10-py3-none-any.whl\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.55.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (61.2.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.38.0)\n",
      "Requirement already satisfied: click in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.4)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.0.1)\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (PEP 517): started\n",
      "  Building wheel for hdbscan (PEP 517): finished with status 'error'\n",
      "Failed to build hdbscan\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\soumya nanditha\\anaconda3\\python.exe' 'C:\\Users\\soumya nanditha\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' build_wheel 'C:\\Users\\SOUMYA~1\\AppData\\Local\\Temp\\tmph6xlntsu'\n",
      "       cwd: C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-install-vsczq41l\\hdbscan_a5f1b0a0619048bba15f2344513efec1\n",
      "  Complete output (40 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-39\n",
      "  creating build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\flat.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\hdbscan_.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\plots.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\prediction.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\robust_single_linkage_.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\validity.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\__init__.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  creating build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_flat.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_hdbscan.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_prediction_utils.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_rsl.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\__init__.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  running build_ext\n",
      "  cythoning hdbscan/_hdbscan_tree.pyx to hdbscan\\_hdbscan_tree.c\n",
      "  C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-build-env-fo1ke6te\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-install-vsczq41l\\hdbscan_a5f1b0a0619048bba15f2344513efec1\\hdbscan\\_hdbscan_tree.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_hdbscan_linkage.pyx to hdbscan\\_hdbscan_linkage.c\n",
      "  C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-build-env-fo1ke6te\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-install-vsczq41l\\hdbscan_a5f1b0a0619048bba15f2344513efec1\\hdbscan\\_hdbscan_linkage.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_hdbscan_boruvka.pyx to hdbscan\\_hdbscan_boruvka.c\n",
      "  C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-build-env-fo1ke6te\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-install-vsczq41l\\hdbscan_a5f1b0a0619048bba15f2344513efec1\\hdbscan\\_hdbscan_boruvka.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_hdbscan_reachability.pyx to hdbscan\\_hdbscan_reachability.c\n",
      "  C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-build-env-fo1ke6te\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-install-vsczq41l\\hdbscan_a5f1b0a0619048bba15f2344513efec1\\hdbscan\\_hdbscan_reachability.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_prediction_utils.pyx to hdbscan\\_prediction_utils.c\n",
      "  C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-build-env-fo1ke6te\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-install-vsczq41l\\hdbscan_a5f1b0a0619048bba15f2344513efec1\\hdbscan\\_prediction_utils.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/dist_metrics.pyx to hdbscan\\dist_metrics.c\n",
      "  C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-build-env-fo1ke6te\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-install-vsczq41l\\hdbscan_a5f1b0a0619048bba15f2344513efec1\\hdbscan\\dist_metrics.pxd\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  building 'hdbscan._hdbscan_tree' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for hdbscan\n",
      "ERROR: Could not build wheels for hdbscan which use PEP 517 and cannot be installed directly\n"
     ]
    }
   ],
   "source": [
    "pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Using cached bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from bertopic) (5.6.0)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from bertopic) (1.4.2)\n",
      "Collecting hdbscan>=0.8.29\n",
      "  Using cached hdbscan-0.8.33.tar.gz (5.2 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from bertopic) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from bertopic) (1.0.2)\n",
      "Collecting sentence-transformers>=0.4.1\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from bertopic) (4.64.0)\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Using cached umap_learn-0.5.4-py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.7.3)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.1.0)\n",
      "Requirement already satisfied: cython<3,>=0.27 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (0.29.28)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2021.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from plotly>=4.7.0->bertopic) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.2.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (0.9.0)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (1.8.0)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.4.0-cp39-none-win_amd64.whl (277 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.3.15)\n",
      "Collecting tokenizers<0.15,>=0.14\n",
      "  Using cached tokenizers-0.14.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from umap-learn>=0.5.0->bertopic) (0.55.1)\n",
      "Collecting pynndescent>=0.5\n",
      "  Using cached pynndescent-0.5.10-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (61.2.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.38.0)\n",
      "Requirement already satisfied: click in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (7.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\soumya nanditha\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.0.1)\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (PEP 517): started\n",
      "  Building wheel for hdbscan (PEP 517): finished with status 'error'\n",
      "Failed to build hdbscan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\soumya nanditha\\anaconda3\\python.exe' 'C:\\Users\\soumya nanditha\\anaconda3\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' build_wheel 'C:\\Users\\SOUMYA~1\\AppData\\Local\\Temp\\tmp70y9lmx5'\n",
      "       cwd: C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-install-9kwgkfg_\\hdbscan_5b6fb30503134ecfbecd653799addfd3\n",
      "  Complete output (40 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-39\n",
      "  creating build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\flat.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\hdbscan_.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\plots.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\prediction.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\robust_single_linkage_.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\validity.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  copying hdbscan\\__init__.py -> build\\lib.win-amd64-cpython-39\\hdbscan\n",
      "  creating build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_flat.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_hdbscan.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_prediction_utils.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_rsl.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\__init__.py -> build\\lib.win-amd64-cpython-39\\hdbscan\\tests\n",
      "  running build_ext\n",
      "  cythoning hdbscan/_hdbscan_tree.pyx to hdbscan\\_hdbscan_tree.c\n",
      "  C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-build-env-4ct5q9q7\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-install-9kwgkfg_\\hdbscan_5b6fb30503134ecfbecd653799addfd3\\hdbscan\\_hdbscan_tree.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_hdbscan_linkage.pyx to hdbscan\\_hdbscan_linkage.c\n",
      "  C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-build-env-4ct5q9q7\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-install-9kwgkfg_\\hdbscan_5b6fb30503134ecfbecd653799addfd3\\hdbscan\\_hdbscan_linkage.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_hdbscan_boruvka.pyx to hdbscan\\_hdbscan_boruvka.c\n",
      "  C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-build-env-4ct5q9q7\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-install-9kwgkfg_\\hdbscan_5b6fb30503134ecfbecd653799addfd3\\hdbscan\\_hdbscan_boruvka.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_hdbscan_reachability.pyx to hdbscan\\_hdbscan_reachability.c\n",
      "  C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-build-env-4ct5q9q7\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-install-9kwgkfg_\\hdbscan_5b6fb30503134ecfbecd653799addfd3\\hdbscan\\_hdbscan_reachability.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/_prediction_utils.pyx to hdbscan\\_prediction_utils.c\n",
      "  C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-build-env-4ct5q9q7\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-install-9kwgkfg_\\hdbscan_5b6fb30503134ecfbecd653799addfd3\\hdbscan\\_prediction_utils.pyx\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  cythoning hdbscan/dist_metrics.pyx to hdbscan\\dist_metrics.c\n",
      "  C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-build-env-4ct5q9q7\\overlay\\Lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\soumya nanditha\\AppData\\Local\\Temp\\pip-install-9kwgkfg_\\hdbscan_5b6fb30503134ecfbecd653799addfd3\\hdbscan\\dist_metrics.pxd\n",
      "    tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "  building 'hdbscan._hdbscan_tree' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for hdbscan\n",
      "ERROR: Could not build wheels for hdbscan which use PEP 517 and cannot be installed directly\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bertopic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install bertopic\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bertopic'"
     ]
    }
   ],
   "source": [
    "!pip install bertopic\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BERTopic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Write your code here\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m topic_model \u001b[38;5;241m=\u001b[39m \u001b[43mBERTopic\u001b[49m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, calculate_probabilities\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m text_data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBreaking: Alien Invasion Imminent, Government Warns\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudy Finds Vaccines Effective in Preventing Disease\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBillionaire Elon Musk to Fund Mission to Mars\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExclusive: Top-Secret Government Files Leaked\u001b[39m\u001b[38;5;124m\"\u001b[39m,]\n\u001b[0;32m      9\u001b[0m topics, probs \u001b[38;5;241m=\u001b[39m topic_model\u001b[38;5;241m.\u001b[39mfit_transform(text_data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BERTopic' is not defined"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
    "\n",
    "text_data = [\"Breaking: Alien Invasion Imminent, Government Warns\",\n",
    "    \"Study Finds Vaccines Effective in Preventing Disease\",\n",
    "    \"Billionaire Elon Musk to Fund Mission to Mars\",\n",
    "    \"Exclusive: Top-Secret Government Files Leaked\",]\n",
    "topics, probs = topic_model.fit_transform(text_data)\n",
    "\n",
    "\n",
    "from bertopic import CoherenceModel\n",
    "\n",
    "best_k = -1\n",
    "best_coherence = -1\n",
    "\n",
    "for k in range(3, 14):  # Adjust the range as needed\n",
    "    coherence_model = CoherenceModel(topic_model, texts=text_data, topics=topics, dictionary=your_dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "    if coherence_score > best_coherence:\n",
    "        best_k = k\n",
    "        best_coherence = coherence_score\n",
    "        \n",
    "top_words_per_topic = topic_model.get_topics()\n",
    "\n",
    "print(\"Optimal number of topics (K):\", best_k)\n",
    "print(\"Topics:\")\n",
    "for i, words in enumerate(top_words_per_topic):\n",
    "    print(f\"Topic {i}:\", words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here (no code needed for this question)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
