{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoumyaNanditha/SoumyaNanditha_INFO5731_-Fall2023/blob/main/Chadalavada_SoumyaNanditha_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 reddits by using a hashtag (you can use any hashtag) from Reddits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696f38e5-9822-4690-d174-21e643a7130e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data from 12500 user reviews collected and saved to film_reviews.csv.\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Define the URL of the IMDb page with user reviews\n",
        "url = 'https://www.imdb.com/title/tt1877830/reviews/?ref_=tt_ql_2'\n",
        "\n",
        "# Initialize empty lists to store data\n",
        "usernames = []\n",
        "ratings = []\n",
        "review_dates = []\n",
        "reviews = []\n",
        "\n",
        "# Loop to collect data from multiple pages (adjust the range accordingly)\n",
        "for page in range(1, 501):  # Collecting from 100 pages, 100 reviews per page\n",
        "    page_url = f\"{url}&start={10 * (page - 1)}\"  # IMDb shows 10 reviews per page\n",
        "\n",
        "    # Send an HTTP GET request to the page\n",
        "    response = requests.get(page_url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve data from page {page}\")\n",
        "        continue\n",
        "\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extract user reviews and related data\n",
        "    user_review_elements = soup.find_all('div', class_='text show-more__control')\n",
        "\n",
        "    for element in user_review_elements:\n",
        "        username = element.find_previous('a', href=True).text\n",
        "        rating = element.find_previous('span', class_='point-scale').text.strip()\n",
        "        review_date = element.find_previous('span', class_='review-date').text.strip()\n",
        "        review_text = element.text.strip()\n",
        "\n",
        "        usernames.append(username)\n",
        "        ratings.append(rating)\n",
        "        review_dates.append(review_date)\n",
        "        reviews.append(review_text)\n",
        "\n",
        "# Create a list of dictionaries to store the data\n",
        "data = []\n",
        "for i in range(len(usernames)):\n",
        "    data.append({\n",
        "        'Username': usernames[i],\n",
        "        'Rating': ratings[i],\n",
        "        'Review Date': review_dates[i],\n",
        "        'Review Text': reviews[i]\n",
        "    })\n",
        "\n",
        "# Write the data to a CSV file\n",
        "with open('film_reviews.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    fieldnames = ['Username', 'Rating', 'Review Date', 'Review Text']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(f\"Data from {len(usernames)} user reviews collected and saved to film_reviews.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4dfa92-9e7f-4fba-d320-d000852f20d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to cleaned_film_reviews.csv.\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import csv\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the NLTK Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Initialize spaCy for lemmatization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a function for text cleaning\n",
        "def clean_text(text):\n",
        "    # Remove special characters, punctuation, and numbers\n",
        "    text = re.sub(r'[^A-Za-z]+', ' ', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
        "\n",
        "    # Convert to lowercase\n",
        "    words = [word.lower() for word in words]\n",
        "\n",
        "    # Apply stemming\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # Apply lemmatization\n",
        "    doc = nlp(\" \".join(words))\n",
        "    words = [token.lemma_ for token in doc]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Read the CSV file with the original data\n",
        "input_file = 'film_reviews.csv'\n",
        "output_file = 'cleaned_film_reviews.csv'\n",
        "\n",
        "with open(input_file, 'r', newline='', encoding='utf-8') as csv_file:\n",
        "    reader = csv.DictReader(csv_file)\n",
        "    rows = list(reader)\n",
        "\n",
        "# Create a new list of dictionaries with cleaned data\n",
        "cleaned_data = []\n",
        "for row in rows:\n",
        "    cleaned_text = clean_text(row['Review Text'])\n",
        "    row['Cleaned Review'] = cleaned_text\n",
        "    cleaned_data.append(row)\n",
        "\n",
        "# Write the cleaned data to a new CSV file\n",
        "with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    fieldnames = ['Username', 'Rating', 'Review Date', 'Review Text', 'Cleaned Review']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(cleaned_data)\n",
        "\n",
        "print(f\"Cleaned data saved to {output_file}.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0476f1f-b855-4f11-d39d-6b9850554b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Constituency Parsing Tree:\n",
            "detect [ROOT] -> batman [compound] -> peak [compound] -> great [amod] -> storylin [compound] -> dark [dobj] -> univer [ccomp] -> come [conj] -> expect [dep] -> dc [compound] -> gloomi [nsubj] -> gritti [compound] -> dark [compound] -> tone [compound] -> film [compound] -> exactli [compound] -> want [nsubj] -> think [ccomp] -> movi [nsubj] -> beauti [nsubj] -> cinematographi [ccomp] -> great [amod] -> score [dobj] -> \n",
            "\n",
            "Dependency Parsing Tree:\n",
            "detect [detect] -> batman [peak] -> peak [dark] -> great [storylin] -> storylin [dark] -> dark [detect] -> univer [detect] -> come [detect] -> expect [detect] -> dc [gloomi] -> gloomi [gritti] -> gritti [want] -> dark [tone] -> tone [exactli] -> film [exactli] -> exactli [want] -> want [think] -> think [expect] -> movi [cinematographi] -> beauti [cinematographi] -> cinematographi [think] -> great [score] -> score [cinematographi] -> \n",
            "\n",
            "Named Entity Counts:\n",
            "PERSON: 95000\n",
            "ORG: 13500\n",
            "GPE: 5500\n",
            "PRODUCT: 500\n",
            "DATE: 4000\n",
            "Noun Count: 609500\n",
            "Verb Count: 271500\n",
            "Adjective Count: 229500\n",
            "Adverb Count: 77000\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Read the cleaned text data\n",
        "df = pd.read_csv('cleaned_film_reviews.csv')\n",
        "\n",
        "# Initialize counts for POS tagging\n",
        "noun_count = 0\n",
        "verb_count = 0\n",
        "adj_count = 0\n",
        "adv_count = 0\n",
        "\n",
        "# Initialize variables for constituency and dependency parsing\n",
        "sample_text = df['Cleaned Review'].iloc[0]  # Take the first sentence as an example\n",
        "\n",
        "# (1) Parts of Speech (POS) Tagging\n",
        "for text in df['Cleaned Review']:\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"NOUN\":\n",
        "            noun_count += 1\n",
        "        elif token.pos_ == \"VERB\":\n",
        "            verb_count += 1\n",
        "        elif token.pos_ == \"ADJ\":\n",
        "            adj_count += 1\n",
        "        elif token.pos_ == \"ADV\":\n",
        "            adv_count += 1\n",
        "\n",
        "# (2) Constituency Parsing and Dependency Parsing (using one sentence as an example)\n",
        "sample_doc = nlp(sample_text)\n",
        "\n",
        "# Constituency Parsing Tree\n",
        "print(\"\\nConstituency Parsing Tree:\")\n",
        "for token in sample_doc:\n",
        "    print(f\"{token.text} [{token.dep_}]\", end=\" -> \")\n",
        "print()\n",
        "\n",
        "# Dependency Parsing Tree\n",
        "print(\"\\nDependency Parsing Tree:\")\n",
        "for token in sample_doc:\n",
        "    print(f\"{token.text} [{token.head.text}]\", end=\" -> \")\n",
        "print()\n",
        "\n",
        "# (3) Named Entity Recognition\n",
        "entities = {\n",
        "    \"PERSON\": 0,\n",
        "    \"ORG\": 0,\n",
        "    \"GPE\": 0,\n",
        "    \"PRODUCT\": 0,\n",
        "    \"DATE\": 0\n",
        "}\n",
        "\n",
        "for text in df['Cleaned Review']:\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in entities:\n",
        "            entities[ent.label_] += 1\n",
        "\n",
        "# Print Named Entity Counts\n",
        "print(\"\\nNamed Entity Counts:\")\n",
        "for entity, count in entities.items():\n",
        "    print(f\"{entity}: {count}\")\n",
        "\n",
        "# Print POS tagging counts\n",
        "print(f\"Noun Count: {noun_count}\")\n",
        "print(f\"Verb Count: {verb_count}\")\n",
        "print(f\"Adjective Count: {adj_count}\")\n",
        "print(f\"Adverb Count: {adv_count}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency parsing tree and Constituency parsing trees play a key role in semantics analysis, sentence structure analysis.Dependency parsing considers each word of sentence as a node and grammatical connections between different words as links.It follows a bottom-up approach.Whereas constituency parsing is based on context-free grammar and it follows a top-down approach.\n",
        "\n",
        "Constituency parsing tree indicates how words in a sentence can be merged/grouped together to form different sentences.It follows a hierarchical model.It can be used in actively identifying and classifying POS in sentences and tagging them and helps in forming clusters of them.\n",
        "\n",
        "Unlike Constituency parsing tree, dependency parsing tree gives more importance to connection between words and identifies various types of dependencies like subject-verb etc..This can prove useful in tasks like information extraction, sentiment analysis, ML etc.. These both are useful to bettter understand the structure of a sentence.\n",
        "\n",
        "in the trees above, from the constituency parsing tree we can see that the root word is detect and it continues with other words which are nodesand other words describe film's storyline, cinematography etc..\n",
        "\n",
        "In the dependancy parsing,it analyses the grammatical structure of sentences.It details the subject-verb structure in sentences.The words like gloomi, gritti indicate syntactic roles of words and describe the dark and gritty nature of film.It provides us understanding of roles of different words in sentences."
      ],
      "metadata": {
        "id": "MWqGFauXyFRA"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}