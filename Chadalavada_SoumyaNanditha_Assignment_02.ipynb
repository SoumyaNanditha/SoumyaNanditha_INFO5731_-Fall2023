{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USSdXHuqnwv9"
   },
   "source": [
    "# **INFO5731 Assignment Two**\n",
    "\n",
    "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWxodXh5n4xF"
   },
   "source": [
    "# **Question 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TenBkDJ5n95k"
   },
   "source": [
    "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
    "\n",
    "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
    "\n",
    "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
    "\n",
    "(3) Collect all the reviews of the top 1000 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
    "\n",
    "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from [Semantic Scholar](https://www.semanticscholar.org).\n",
    "\n",
    "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
    "\n",
    "(6) Collect the top 10000 reddits by using a hashtag (you can use any hashtag) from Reddits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PuFPKhC0m1fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews have been scraped and saved to imdb_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Define the URL of the IMDb page with user reviews\n",
    "url = \"https://www.imdb.com/title/tt6791350/reviews/?ref_=tt_ql_2\"\n",
    "\n",
    "# Create a CSV file to store the data\n",
    "csv_filename = \"imdb_reviews.csv\"\n",
    "csv_file = open(csv_filename, 'w', encoding='utf-8', newline='')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow([\"Title\", \"User\", \"Date\", \"Rating\", \"Review\"])\n",
    "\n",
    "# Function to scrape and save reviews\n",
    "def scrape_reviews(url, csv_writer):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        reviews = soup.find_all(\"div\", class_=\"lister-item-content\")\n",
    "\n",
    "        for review in reviews:\n",
    "            title = review.find(\"a\", class_=\"title\").text.strip()\n",
    "            user = review.find(\"span\", class_=\"display-name-link\").text.strip()\n",
    "            date = review.find(\"span\", class_=\"review-date\").text.strip()\n",
    "            rating = review.find(\"span\", class_=\"rating-other-user-rating\").text.strip()\n",
    "            review_text = review.find(\"div\", class_=\"text\").text.strip().replace(\"\\n\", \" \")\n",
    "            csv_writer.writerow([title, user, date, rating, review_text])\n",
    "\n",
    "# Scrape multiple pages of reviews (adjust the range accordingly)\n",
    "for page_num in range(1, 500):\n",
    "    page_url = f\"{url}&start={page_num * 10}\"\n",
    "    scrape_reviews(page_url, csv_writer)\n",
    "\n",
    "csv_file.close()\n",
    "print(\"Reviews have been scraped and saved to\", csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfpMRCrRwN6Z"
   },
   "source": [
    "# **Question 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dCQEbDawWCw"
   },
   "source": [
    "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
    "\n",
    "(1) Remove noise, such as special characters and punctuations.\n",
    "\n",
    "(2) Remove numbers.\n",
    "\n",
    "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
    "\n",
    "(4) Lowercase all texts\n",
    "\n",
    "(5) Stemming.\n",
    "\n",
    "(6) Lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\soumya nanditha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vATjQNTY8buA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\soumya\n",
      "[nltk_data]     nanditha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\soumya\n",
      "[nltk_data]     nanditha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\soumya\n",
      "[nltk_data]     nanditha\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data has been cleaned and saved to imdb_reviews_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the stopwords, stemmer, and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize the text and remove stopwords\n",
    "    words = [word for word in text.lower().split() if word not in stop_words]\n",
    "    # Stem and lemmatize each word\n",
    "    words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words]\n",
    "    # Join the cleaned words back into a string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Load the CSV data into a pandas DataFrame\n",
    "csv_filename = \"imdb_reviews.csv\"\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Clean and preprocess the 'Review' column\n",
    "df['Cleaned Review'] = df['Review'].apply(clean_text)\n",
    "\n",
    "# Save the DataFrame with cleaned data to a new CSV file\n",
    "cleaned_csv_filename = \"imdb_reviews_cleaned.csv\"\n",
    "df.to_csv(cleaned_csv_filename, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Text data has been cleaned and saved to\", cleaned_csv_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5mmYIfN8eYV"
   },
   "source": [
    "# **Question 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsi2y4z88ngX"
   },
   "source": [
    "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes:\n",
    "\n",
    "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
    "\n",
    "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
    "\n",
    "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQKnPjPDHJHr"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWOtvT2rHNWy"
   },
   "source": [
    "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
